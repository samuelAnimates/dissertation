[["index.html", "Artificial Intelligence &amp; Health Literacy: Opportunities and Challenges Chapter 1 Overview 1.1 Calls to Action 1.2 Key Terms", " Artificial Intelligence &amp; Health Literacy: Opportunities and Challenges Samuel R. Mendez 2025-05-17 Chapter 1 Overview Hi! I’m Samuel Mendez. I just got my PhD from the Department of Social and Behavioral Sciences at Harvard’s School of Public Health. I studied how we might use artificial intelligence (AI) to make health communication clearer. This site shares some lessons learned from my dissertation research. More detailed info will come in published journal articles. But in the mean time, I hope you find this site useful! 1.1 Calls to Action In public health, we’re facing a chronic lack of funding, mass layoffs, and budget cuts. At the same time, we’re facing risks like measles, bird flu, and COVID. Clear communication is more important now than ever before. And AI promises to help. To be clear, you can use AI to try to address communication problems. But it’s hard to know which problems it will actually solve. And it’s even harder to predict the effects of model updates. That means you’ll need to put in more work to review and test AI outputs. And it still won’t fix root issues like a lack of public trust or political support. If you’re working in health communication and planning to use AI: Apply AI to narrow tasks that you can track over time. Hire health literacy experts to review AI outputs. Weigh the long-term benefits of investing limited funds in community health workers instead. 1.2 Key Terms 1.2.1 What is Health Literacy? “Health literacy” involves more than just people’s reading skills. As Healthy People 2030 defines it, there are 2 facets of health literacy: Personal health literacy: how well people can find and use health info. Organizational health literacy: how well health organizations provide people with findable, usable health info. Assessing health communication is one way to assess organizational health literacy. Beyond reading grade levels, you can use more robust tools like the CDC Clear Communication Index. 1.2.2 What is AI? “AI” is a moving target. Tech like spam filters, autofill, and navigation apps are examples of everyday AI in action. But they aren’t what the term “AI” brings to mind these days. Melanie Mitchell covers this history of shifting definitions in her book Artificial Intelligence: A Guide for Thinking Humans. In this project, I adapt her description of the practical goals of “AI” as a field: Artificial Intelligence: computer programs meant to perform tasks we normally expect people to do This definition also shows why “AI” is a moving target. We can get used to computers doing certain tasks. Then, after a while, we won’t expect people to do them. For example: finding the quickest route to a new restaurant. "],["using-supervised-learning.html", "Chapter 2 Using Supervised Learning 2.1 Background 2.2 Research Findings 2.3 Recommendations", " Chapter 2 Using Supervised Learning This page describes the results of a project training supervised learning models to scale up the CDC Clear Communication Index. It shows how this technique can help us assess health communication at scale. But it also shows this technique’s limits. It takes a lot of work to figure out what tasks it will be good at. And you still have to decide how good is good enough, given the labor and costs required. 2.1 Background 2.1.1 What is supervised learning? “Supervised learning” is when you use labeled data and an algorithm to predict labels on new data. You might also hear this referred to as “supervised machine learning”. 2.1.2 What kinds of tasks can someone do through supervised learning? Supervised learning is good for “classification tasks”, where you want to sort things into groups. Questions with “Yes” or “No” answers are a common type of classification task. Example: you might want to know whether Tweets have a call to action. You can label Tweets with “Yes” or “No” labels yourself. Then, you can use them to train a model to predict the labels on new Tweets. 2.1.3 How might supervised learning support health literacy? In the example above, we wanted to know whether Tweets had a call to action. This is one of the questions from the CDC Clear Communication Index. We could use supervised learning to create models to answer all of the other questions from the Index too. This could help make software that could automatically give us the results of a health literacy assessment. 2.1.4 How do you train a supervised learning model? Choose a dataset that is similar to what you want to apply your model to in the future. Then label it by hand. Then run the labeled data through your chosen algorithm. In our case, we used 1900 Tweets from US state public health agencies. Then 1 of 2 expert raters assigned “Yes” or “No” labels to them based on questions from the CDC Clear Communication Index. Each Index label was its own task that got its own model. For each task, we ran the labeled data through an algorithm. The result was a trained model that could predict labels on that item for new Tweets. 2.1.5 How do you know if a supervised learning model is good enough? You label a fresh dataset. Then you have the model label it too. Calculate a performance metric that describes how well the models’ labels match yours. Keep in mind: “good enough” isn’t just about scores. Weigh the real-world costs of wrong labels. Weigh the effort of testing the model over time. In our case, 2 expert raters applied questions from the CDC Clear Communication Index to 300 pandemic Tweets from US state public health agencies. A 3rd rater resolved conflicts to make gold-standard labels. We then had the models label the Tweets too. We used AUC as our performance metric. We liked that it captures info about true “Yes” labels and true “No” labels. 2.2 Research Findings 2.2.1 Do some types of models perform better than others? Yes, but it depends on the task at hand. We trained 2 different kinds of models to label Tweets based on 6 items from the CDC Clear Communication Index. We created Lasso regression models, which used word counts to predict labels. We also fine-tuned BERT to create models that used neural networks to predict labels based on how words appear in context. We found BERT models had a higher AUC than Lasso regression models on all 6 tasks. But choosing between the two wouldn’t always be so easy. Example: we observed the largest difference between both approaches when labeling Tweets as either using active voice or not. This difference is likely big enough to rule out the Lasso regression model. (But this doesn’t mean that the BERT model performs well enough for real-world use.) Figure 2.1: The difference between both models’ performance was the biggest we observed. In contrast, we observed the smallest difference between both approaches when labeling Tweets as either having info about risk or not. This difference is likely small enough that other factors like model explainability would help you choose between the two. Figure 2.2: The difference between both models’ performance was much smaller here. And both performed pretty well. 2.2.2 Are some tasks better suited to supervised learning? Yes. One way to think about this is in terms of performance metrics. Example: we observed the lowest AUC for both approaches when labeling Tweets as using common words or not. In contrast, we observed some of the highest AUC values when labeling tweets as having a call to action or not. You can also think in terms of inter-rater agreement. We used kappa to assess agreement between our raters as we created gold-standard labels. We saw low agreement on the “common words” task. Thus this task was not a good fit for either type of model, because our own raters couldn’t quite agree on what the label meant. In contrast, the “call to action” item had the highest agreement. Thus this task is a better fit for supervised learning. Figure 2.3: Inter-rater agreement on the ‘Call to Action’ task met a standard benchmark of kappa=0.7. The ‘Common Words’ task was far below this benchmark 2.2.3 Are some messages better suited to supervised learning? Yes. To figure this out, we compared 2 groups of Tweets. One group received correct labels across all tasks from all models. The other group received at least 2 incorrect labels across all tasks and all models. One type of Tweet that all models performed well on was a COVID case count update. This mirrors what the expert raters observed while they labeled data themselves. This type of Tweet was common enough that they could create their own rules about how to label them. Here is an example: COVID-19 case summary for Colorado (Nov 5). 121,006 cases 1,287,081 people tested 1,350 outbreaks 2,353 deaths among cases 2,153 deaths due to COVID-19 One type of Tweet that models performed poorly on was a “current events” Tweet. Here is an example: This year it is important to have Thanksgiving meals with only those who live within your household to limit the spread of COVID-19. Utilizing virtual communications is one way to still feel connected to those you are thankful for. This seems to reflect the fact that these kinds of Tweets used words that most other labeled Tweets did not. This reflects a weakness of supervised learning. It might work best for messages you could template because they are so common. As such, your time might be better spent making and sharing templates rather than training models. 2.3 Recommendations 2.3.1 For research Supervised learning can help scale up health literacy research. But it’s hard to know ahead of time where it will be useful. So, create some gold-standard labels like we did. Find out which tasks already have high inter-rater agreement. This way, you can focus on training models that output labels you can interpret. 2.3.2 For practice Label short messages by hand. Our research process shows you might not even need supervised learning models if your messages are short. Each of our raters worked 5 hours per week to label over 1,000 Tweets in just under 3 months. This total number made sense when we were looking at all US state health agencies. For just one organization, you can label far fewer to gain similar insights. 2.3.3 For policy Don’t make decisions based on performance metrics alone. Yes, we looked at performance metrics to figure out which models were more suited to our tasks. But we also looked at inter-rater agreement to figure out which tasks were suited to modeling. We looked at individual Tweets to figure out which kinds of messages were suited to this approach. These insights are more valuable together than they are alone. "],["using-generative-ai.html", "Chapter 3 Using Generative AI 3.1 Background 3.2 Research Findings 3.3 Recommendations", " Chapter 3 Using Generative AI This page describes the results of our project using ChatGPT to scale up the CDC Clear Communication Index. We didn’t focus on whether ChatGPT performs well as a health literacy tool. Instead, we focused on the long-term impact of using it as one. Our results show that you can’t predict how tools like ChatGPT will perform over time. So, you need to hire health literacy experts and community health workers to assess its outputs. Not just once, but for as long as you are using it. 3.1 Background 3.1.1 What is generative AI? “Generative AI” is an approach to artificial intelligence. It focuses on making new content based on patterns in old content. A popular example is ChatGPT. 3.1.2 How can I use generative AI for health communication? That depends on who you ask. A fan might say that your imagination is the only limit. Example: you can prompt it to provide health literacy feedback on your writing. It wasn’t designed for that task. But its massive training dataset might make it flexible enough to do so. A critic might say it’s not at all appropriate for public health. Example: it has environmental costs. It relies on stolen content. And it just makes things up. 3.1.3 How do I know whether AI-generated content is good enough? You have to test it to find out. Define a specific task you want to use it on. Example: decide whether a Tweet includes a call to action. Prompt the AI system to do that task on lots of different Tweets. Then compare its outputs to your own decisions for each Tweet. Calculate a performance metric that describes how well the models’ outputs match your own decisions. 3.2 Research Findings 3.2.1 Will minor model updates affect performance? It’s hard to say. We had 2 expert raters assess 260 US public health agency Tweets, using 6 items from the CDC Clear Communication. Example: does this material have a main message statement? Or: does this material include a behavioral recommendation? A 3rd rater resolved any conflicts to create our gold-standard data. We then prompted the November 2023 version of ChatGPT 3.5 to apply the same items to the same Tweets. We did the same with the January 2024 version. We used MCC as our performance metric to describe how well each ChatGPT model aligned with our gold-standard data. For some tasks, both model versions were about the same. But for others there was a clear difference. Figure 3.1: Our results show no clear pattern in performance. This means even so-called minor updates to an AI model need to be tested like brand new models. 3.2.2 Will more advanced models perform better? Not necessarily. We prompted GPT 4-turbo and 4-o to apply the same items to the same Tweets as before. Again, we used MCC to describe how well each model aligned with our gold-standard data. The 4-o model is touted as the most advanced. But it did not always perform the best on these health communication tasks. Sometimes a GPT-3.5 model was the best. Sometimes the 4-turbo model was the best. There was no clear pattern. Figure 3.2: Our results show now clear pattern in performance. This means paing for a more advanced model can have mixed effects on health communication tasks. 3.3 Recommendations 3.3.1 For research Our results cast doubt on how relevant tech industry benchmarks are for health communication. So, test large language models against a custom gold-standard validation dataset. Make your dataset so you can also draw insights from it without the use of AI. 3.3.2 For practice Our results show that generative AI does not work the same across tasks we might think are similar. Example: after training someone, you might expect them to spot calls to action about as well as behavioral recommendations. But that wasn’t the case with ChatGPT. So, apply ChatGPT to specific tasks you know well. That way, you can judge its outputs over time without too much effort. 3.3.3 For policy Our results highlight how generative AI can be unpredictable. So, health organizations should use generative AI only if they can commit to funding a transparent, long-term evaluation plan. This plan should cover the entire timeline of its expected use, across all use cases. Plain-language evaluation reports should be available to all communities served through these tools. "],["factor-framework-to-assess-ai.html", "Chapter 4 4-Factor Framework to Assess AI 4.1 Explainability 4.2 Flexibility 4.3 Performance 4.4 Fairness 4.5 Cycles", " Chapter 4 4-Factor Framework to Assess AI When we judge whether a tool is suitable for a public health job, we look beyond the numbers. We ask how it works. What does it need to keep running? Does it help close health equity gaps? How fast can we tell if something goes wrong? And what does it take to get it back on track? This is already complex. And AI adds even more steps to this process. We expect to update a typical AI tool over time. We expect it to change as real-world data change. This means we can’t just judge whether an AI tool is suitable once. We need to keep assessing it over time. To help account for this, we introduce the 4-Factor Framework to Assess AI for Health Communication. The 4-Factor Framework defines suitability in terms of explainability, flexibility, performance, and fairness. It prompts you to think about how these factors might change through cycles of model training and implementation. It prompts you to consider how a broader context of inequities, human decisions, and institutional priorities shape this cycle. 4.1 Explainability Some models are easier to explain than others. Judging this requires knowledge of the model’s structure. Example: Lasso regression uses variables with clearly defined relationships to the outputs. So, you can explain how the whole model works. But fine-tuned BERT models are black boxes. They rely on complex patterns of words in context. You can start to explain single outputs. But not the whole model. Some tasks are easier to explain than others too. Judging this requires access to model training and validation data. Example: inter-rater agreement was low on the task the model was trained to perform. This model would be hard to explain because humans couldn’t agree on what the task meant to begin with. Learn more about model explainability. 4.2 Flexibility Some models can adapt to more complex patterns than others. Judging this requires knowledge of how a model works. Example: regression models have rigid structures that require strict assumptions about training data. In contrast, fine-tuned BERT models adapt to many kinds of relationships between in training data. However, these relationships can be so complex that humans can’t understand them. Learn more about model flexibility. 4.3 Performance Some models do a task better than others. Judging this requires access to model validation data. Or making your own validation data set. In short, you need data with gold-standard labels. Then you can to test if the model outputs the same labels. You can use performance metrics to describe how often model outputs match the gold standard. Learn more about model performance. 4.4 Fairness Some models might work better in certain contexts than others. Judging this might require making new validation datasets. Example: a model that identifies Tweets with calls to action. To judge its fairness, you might create a validation dataset of Tweets from health agencies with lower budgets. Does the model perform as well on those Tweets as does in other contexts? You might also judge model fairness by exploring model training and validation data. How well do those datasets match the communities you’re interested in? Learn more about model fairness. 4.5 Cycles The 4-Factor Framework asks you to think about the broader context. And its interactions with your cycles of model training and implementation. Here are some questions to help you get started. 4.5.1 Human decisions Who trained the models? What was their background? Why did they do it? Who decides whether a model is suitable? Why are they in this position? Where did the training data come from? Who collected it? For what purpose? Did model training require labeled data? Who labeled it? What was their background? Who is responsible for the models over time? Why are they in this position? 4.5.2 Communication inequities Who do model training datasets include? Who’s missing? Who do we expect the model to serve? How well do model developers represent the communities you want to reach? What about model evaluators? What about the users of AI tools? 4.5.3 Institutional priorities Where did people work or study while they trained the models? Who benefits from a model performing well? How? What was the goal of training models? "],["conclusion.html", "Chapter 5 Conclusion 5.1 Calls to Action 5.2 Let’s Connect 5.3 Acknowledgements", " Chapter 5 Conclusion Overall, my dissertation shows the promise and limits of using AI in public health. It’s not an automatic fix. Like any public health tool, it has its limits. Chapter 2 covered supervised learning. It showed the value of fine-tuning AI for health communication tasks. You can build models for narrow goals, like assessing health literacy demands. That makes them easier to test. Open-source tools with clear documentation helped. They made the models easier to interpret and apply in public health settings. Chapter 3 covered generative AI. It showed how much work it takes to evaluate AI tools over time. And why that matters. Tools like ChatGPT are unpredictable. They’re not trained for your health literacy tasks. So you need clear, narrow use cases. You can fine-tune them. But that shifts the process back toward what we saw in Chapter 2. More work. Less magic. Chapter 4 introduced the 4-Factor framework to assess AI. It looks beyond performance. It adds flexibility, explainability, and fairness. These are key concerns in public health, which seeks to advance health equity and serve all communities. 5.1 Calls to Action If you’re working in health communication and planning to use AI: Apply AI to narrow tasks that you can track over time. Hire health literacy experts to review AI outputs. Weigh the long-term benefits of investing limited funds in community health workers instead. 5.2 Let’s Connect As you might guess, I am online a lot. If you’re looking for more info on public health, media, and technology: Check out my professional portfolio at samuelanimates.com Subscribe to my Health Literacy Right Now newsletter Use my Public Health Starter Pack to join a growing network on BlueSky Follow my Public Health  Feed on BlueSky 5.3 Acknowledgements This site does not contain the full breadth of research methods and findings of my dissertation. I will share those in published journal articles. In the mean time, I would like to acknowledge my study coauthors. My dissertation committee: Karen Emmons, Sebastian Munoz-Najar, and K. Vish Viswanath. Research assistants on these papers: Zichao Li and Elissa Scherer. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
